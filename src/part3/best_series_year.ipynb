{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark version:: 3.5.3\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime\n",
    "from delta import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType\n",
    "import findspark\n",
    "spark=None\n",
    "\n",
    "def initiate_spark_session():\n",
    "    \"\"\"\n",
    "    To instantiate the Spark session object with\n",
    "    required libs to read and manipulate data from S3\n",
    "    \"\"\"\n",
    "    findspark.init()\n",
    "\n",
    "    # Create a Spark session\n",
    "    # Provide valid AWS Access Key Id and Value\n",
    "    global spark \n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Find Best Series Year\") \\\n",
    "        .master('local') \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"<AWS_Access_Key_Id>\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"<AWS_Access_Key_Value>\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.1\").getOrCreate()\n",
    "    \n",
    "     \n",
    "    # my_packages = [\"org.apache.hadoop:hadoop-aws:3.3.1\"]\n",
    "\n",
    "    # spark = configure_spark_with_delta_pip(builder, extra_packages=my_packages).getOrCreate()\n",
    "    print(\"spark version::\", spark.version)\n",
    "\n",
    "initiate_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+------+\n",
      "|series_id  |year|value |\n",
      "+-----------+----+------+\n",
      "|PRS30006011|2022|4.9   |\n",
      "|PRS30006012|2020|6.5   |\n",
      "|PRS30006013|2009|99.726|\n",
      "|PRS30006021|2021|5.3   |\n",
      "|PRS30006022|1996|6.7   |\n",
      "|PRS30006023|1998|99.992|\n",
      "|PRS30006031|2021|9.9   |\n",
      "|PRS30006032|2020|8.9   |\n",
      "|PRS30006033|2020|99.84 |\n",
      "|PRS30006061|2021|9.5   |\n",
      "|PRS30006062|2004|9.7   |\n",
      "|PRS30006063|2017|99.204|\n",
      "|PRS30006081|1995|9.0   |\n",
      "|PRS30006082|1995|9.0   |\n",
      "|PRS30006083|2008|99.78 |\n",
      "|PRS30006091|2010|9.0   |\n",
      "|PRS30006092|2009|9.6   |\n",
      "|PRS30006093|2018|99.954|\n",
      "|PRS30006101|2000|8.1   |\n",
      "|PRS30006102|2004|9.9   |\n",
      "+-----------+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def read_bls_data():\n",
    "    \"\"\"\n",
    "    To read the BLS data from S3, pre-process and \n",
    "    return the transformed dataframe\n",
    "    \"\"\"\n",
    "    # S3 bucket and file path\n",
    "    s3_bucket = \"rearc-assessment\"\n",
    "    s3_file_path = f\"s3a://{s3_bucket}/bls/pr.data.0.Current\"\n",
    "\n",
    "    # Read BLS data CSV file from S3\n",
    "    bls_df = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .option(\"delimiter\", \"\\t\") \\\n",
    "        .load(s3_file_path)\n",
    "    \n",
    "    # Trim leading and tralining spaces in column names and values\n",
    "    trimmed_columns = [col.strip() for col in bls_df.columns]  # Use strip() to trim spaces\n",
    "    trimmed_df = bls_df.toDF(*trimmed_columns)\n",
    "    trimmed_df=trimmed_df.select([trim(col).alias(col) for col in trimmed_df.columns])\n",
    "\n",
    "    qtr_window = Window.partitionBy(\"series_id\", \"year\").orderBy(desc(\"value\"))\n",
    "    year_window = Window.partitionBy(\"series_id\").orderBy(desc(\"value\"))\n",
    "\n",
    "    final_df = trimmed_df.withColumn(\"qtr_row_no\", row_number().over(qtr_window)).filter(col(\"qtr_row_no\") == 1)\\\n",
    "        .withColumn(\"year_row_no\", row_number().over(year_window)).filter(col(\"year_row_no\") == 1)\n",
    "    # Sort and Display the DataFrame schema and data\n",
    "    final_df.orderBy(col(\"series_id\").asc()).drop(\"year_row_no\", \"qtr_row_no\", \"period\", \"footnote_codes\").show(truncate=False)\n",
    "    return bls_df # Return the source dataframe for Part 3.c\n",
    "\n",
    "bls_df=read_bls_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------------+\n",
      "|Mean Population|Standard Deviation|\n",
      "+---------------+------------------+\n",
      "|3.17437383E8   |4257089.54152933  |\n",
      "+---------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def read_usa_population_data():\n",
    "    \"\"\"\n",
    "    To read the USA population data from S3, pre-process and \n",
    "    return the transformed dataframe\n",
    "    \"\"\"\n",
    "    # S3 bucket and file path\n",
    "    s3_bucket = \"rearc-assessment\"\n",
    "    s3_file_path = f\"s3a://{s3_bucket}/usa_data/usa_population_data.json\"\n",
    "\n",
    "    # USA data population schema\n",
    "    data_schema = StructType([\n",
    "        StructField(\"ID Nation\", StringType(), True),\n",
    "        StructField(\"Nation\", StringType(), True),\n",
    "        StructField(\"ID Year\", IntegerType(), True),\n",
    "        StructField(\"Year\", StringType(), True),\n",
    "        StructField(\"Population\", LongType(), True),\n",
    "        StructField(\"Slug Nation\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    # Read USA data population JSON file from S3\n",
    "    pop_df = spark.read.schema(data_schema).option(\"multiline\", \"true\")\\\n",
    "        .option(\"encoding\", \"UTF-8\").json(s3_file_path)\n",
    "    \n",
    "    # Filter the DataFrame for years 2013 to 2018. Since the records are available from 2013.\n",
    "    filtered_df = pop_df.filter((col(\"Year\") >= \"2013\") & (col(\"Year\") <= \"2018\"))\n",
    "\n",
    "    # Calculate mean and standard deviation of the population\n",
    "    stats_df = filtered_df.agg(\n",
    "        mean(col(\"Population\")).alias(\"Mean Population\"),\n",
    "        stddev(col(\"Population\")).alias(\"Standard Deviation\")\n",
    "    )\n",
    "    stats_df.show(truncate=False)\n",
    "    return pop_df # Return the source dataframe for Part 3.c\n",
    "\n",
    "pop_df=read_usa_population_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----+------+------------+----------+\n",
      "|series_id        |year|period|       value|Population|\n",
      "+-----------------+----+------+------------+----------+\n",
      "|PRS30006011      |2013|Q01   |1.4         |311536594 |\n",
      "|PRS30006011      |2013|Q02   |0.4         |311536594 |\n",
      "|PRS30006011      |2013|Q03   |0.3         |311536594 |\n",
      "|PRS30006011      |2013|Q04   |0.7         |311536594 |\n",
      "|PRS30006011      |2013|Q05   |0.7         |311536594 |\n",
      "|PRS30006011      |2014|Q01   |0.6         |314107084 |\n",
      "|PRS30006011      |2014|Q02   |1.1         |314107084 |\n",
      "|PRS30006011      |2014|Q03   |1.5         |314107084 |\n",
      "|PRS30006011      |2014|Q04   |1.9         |314107084 |\n",
      "|PRS30006011      |2014|Q05   |1.3         |314107084 |\n",
      "|PRS30006011      |2015|Q01   |1.5         |316515021 |\n",
      "|PRS30006011      |2015|Q02   |1.5         |316515021 |\n",
      "|PRS30006011      |2015|Q03   |1.4         |316515021 |\n",
      "|PRS30006011      |2015|Q04   |0.8         |316515021 |\n",
      "|PRS30006011      |2015|Q05   |1.3         |316515021 |\n",
      "|PRS30006011      |2016|Q01   |0.6         |318558162 |\n",
      "|PRS30006011      |2016|Q02   |0.1         |318558162 |\n",
      "|PRS30006011      |2016|Q03   |0.1         |318558162 |\n",
      "|PRS30006011      |2016|Q04   |-0.4        |318558162 |\n",
      "|PRS30006011      |2016|Q05   |0.1         |318558162 |\n",
      "+-----------------+----+------+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def join_bls_pop_data(bls_df, pop_df):\n",
    "    \"\"\"\n",
    "    To join the BLS and Population data to derive the final output\n",
    "    \"\"\"\n",
    "    join_df = bls_df.filter(col(\"year\") >=2013).join(pop_df, bls_df[\"year\"] == pop_df[\"year\"], \"left\")\\\n",
    "        .select(bls_df[\"*\"], pop_df[\"Population\"]).drop(\"footnote_codes\").show(truncate=False)\n",
    "    return join_df\n",
    "\n",
    "join_bls_pop_data(bls_df, pop_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
